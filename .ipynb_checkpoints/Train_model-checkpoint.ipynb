{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf806a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.29.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m486.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72ff648-e206-4382-9fa0-687ea07fa147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:21.124595Z",
     "iopub.status.busy": "2023-07-11T00:37:21.124227Z",
     "iopub.status.idle": "2023-07-11T00:37:32.221350Z",
     "shell.execute_reply": "2023-07-11T00:37:32.220314Z",
     "shell.execute_reply.started": "2023-07-11T00:37:21.124558Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizerFast\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "train_data, test_data = load_dataset(\"Hello-SimpleAI/HC3\", name = 'all',  split=['train[:70%]', 'train[70%:]'])\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7e512-26e1-4f38-9e56-6a55595710c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:32.223513Z",
     "iopub.status.busy": "2023-07-11T00:37:32.223008Z",
     "iopub.status.idle": "2023-07-11T00:37:32.229390Z",
     "shell.execute_reply": "2023-07-11T00:37:32.228505Z",
     "shell.execute_reply.started": "2023-07-11T00:37:32.223486Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27094945-265a-4087-a9ca-5f20cbed6980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:32.233277Z",
     "iopub.status.busy": "2023-07-11T00:37:32.233028Z",
     "iopub.status.idle": "2023-07-11T00:37:32.239151Z",
     "shell.execute_reply": "2023-07-11T00:37:32.238374Z",
     "shell.execute_reply.started": "2023-07-11T00:37:32.233255Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639963bf-eeec-4819-9987-4e372c369a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:32.241981Z",
     "iopub.status.busy": "2023-07-11T00:37:32.241760Z",
     "iopub.status.idle": "2023-07-11T00:37:32.246562Z",
     "shell.execute_reply": "2023-07-11T00:37:32.245668Z",
     "shell.execute_reply.started": "2023-07-11T00:37:32.241959Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b9401-2e2e-4f60-b3ed-74098bfe3073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:32.250563Z",
     "iopub.status.busy": "2023-07-11T00:37:32.250321Z",
     "iopub.status.idle": "2023-07-11T00:37:34.953638Z",
     "shell.execute_reply": "2023-07-11T00:37:34.952641Z",
     "shell.execute_reply.started": "2023-07-11T00:37:32.250540Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    data = []\n",
    "    for item in dataset:\n",
    "        for ans in item['human_answers']:\n",
    "            data.append({'text': ans, 'label': 'human'})\n",
    "        for ans in item['chatgpt_answers']:\n",
    "            data.append({'text': ans, 'label': 'chatgpt'})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_df = prepare_dataset(train_data)\n",
    "test_df = prepare_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a34d61-3587-4ca1-af94-2f4e0972eed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:34.957357Z",
     "iopub.status.busy": "2023-07-11T00:37:34.957110Z",
     "iopub.status.idle": "2023-07-11T00:37:34.970997Z",
     "shell.execute_reply": "2023-07-11T00:37:34.970083Z",
     "shell.execute_reply.started": "2023-07-11T00:37:34.957333Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd54a7a-0aeb-48b7-9710-cbe6676a1762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:34.974579Z",
     "iopub.status.busy": "2023-07-11T00:37:34.974281Z",
     "iopub.status.idle": "2023-07-11T00:38:04.825544Z",
     "shell.execute_reply": "2023-07-11T00:38:04.824784Z",
     "shell.execute_reply.started": "2023-07-11T00:37:34.974555Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a label mapping \n",
    "label2id = {'human':0, 'chatgpt':1}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def encode_examples(df):\n",
    "    encodings = tokenizer(df['text'].tolist(), truncation=True, padding='longest', max_length=512, return_tensors='pt')\n",
    "    labels = df['label'].map(label2id).tolist()\n",
    "    return {**encodings, 'labels': labels}\n",
    "\n",
    "train_encodings = encode_examples(train_df)\n",
    "test_encodings = encode_examples(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb4e60-a6ec-48b3-aa9f-2196f0b0385d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:38:04.829159Z",
     "iopub.status.busy": "2023-07-11T00:38:04.828728Z",
     "iopub.status.idle": "2023-07-11T00:38:04.834880Z",
     "shell.execute_reply": "2023-07-11T00:38:04.834145Z",
     "shell.execute_reply.started": "2023-07-11T00:38:04.829134Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataset class \n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items() if key != 'labels'}\n",
    "        item['labels'] = torch.tensor(self.encodings['labels'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(train_encodings)\n",
    "test_dataset = MyDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b316a9-4e84-437f-9286-67da6d9c350b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:38:04.839961Z",
     "iopub.status.busy": "2023-07-11T00:38:04.839497Z",
     "iopub.status.idle": "2023-07-11T00:38:08.047891Z",
     "shell.execute_reply": "2023-07-11T00:38:08.047088Z",
     "shell.execute_reply.started": "2023-07-11T00:38:04.839935Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup Trainer for training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    per_device_train_batch_size=16,  # Reduce batch size\n",
    "    gradient_accumulation_steps=4,  # Add gradient accumulation\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Specify the configuration\n",
    "config = RobertaConfig.from_pretrained('roberta-base')\n",
    "config.num_labels = len(label2id)  # Number of classes for classification\n",
    "config.gradient_checkpointing = True  # Enable gradient checkpointing\n",
    "\n",
    "# Create a new RoBERTa model for sequence classification\n",
    "model = RobertaForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa684d6-2549-456b-b0b3-5b02efdba106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:38:08.051425Z",
     "iopub.status.busy": "2023-07-11T00:38:08.050992Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup Trainer for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          \n",
    "#     per_device_train_batch_size=64,\n",
    "#     num_train_epochs=3,  # You might want to increase this\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                 \n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,   \n",
    "    eval_dataset=test_dataset,          \n",
    "    data_collator=DataCollatorWithPadding(tokenizer), \n",
    "    tokenizer=tokenizer,  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c17823-9cd3-4bcb-af26-8b26209907c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

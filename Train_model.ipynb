{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72ff648-e206-4382-9fa0-687ea07fa147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:21.124595Z",
     "iopub.status.busy": "2023-07-11T00:37:21.124227Z",
     "iopub.status.idle": "2023-07-11T00:37:32.221350Z",
     "shell.execute_reply": "2023-07-11T00:37:32.220314Z",
     "shell.execute_reply.started": "2023-07-11T00:37:21.124558Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset hc3 (/root/.cache/huggingface/datasets/Hello-SimpleAI___hc3/all/1.1.0/5af5910f9f3fe7aace30e32ad4c1ab776ca08183d00e9b2a091308549f69f683)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a0e8b822b04238863185c0ef2d8116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizerFast\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "train_data, test_data = load_dataset(\"Hello-SimpleAI/HC3\", name = 'all',  split=['train[:70%]', 'train[70%:]'])\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c7e512-26e1-4f38-9e56-6a55595710c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:32.223513Z",
     "iopub.status.busy": "2023-07-11T00:37:32.223008Z",
     "iopub.status.idle": "2023-07-11T00:37:32.229390Z",
     "shell.execute_reply": "2023-07-11T00:37:32.228505Z",
     "shell.execute_reply.started": "2023-07-11T00:37:32.223486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'human_answers': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'chatgpt_answers': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'source': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27094945-265a-4087-a9ca-5f20cbed6980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:32.233277Z",
     "iopub.status.busy": "2023-07-11T00:37:32.233028Z",
     "iopub.status.idle": "2023-07-11T00:37:32.239151Z",
     "shell.execute_reply": "2023-07-11T00:37:32.238374Z",
     "shell.execute_reply.started": "2023-07-11T00:37:32.233255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'question': 'Why is every book I hear about a \" NY Times # 1 Best Seller \" ? ELI5 : Why is every book I hear about a \" NY Times # 1 Best Seller \" ? Should n\\'t there only be one \" # 1 \" best seller ? Please explain like I\\'m five.',\n",
       " 'human_answers': ['Basically there are many categories of \" Best Seller \" . Replace \" Best Seller \" by something like \" Oscars \" and every \" best seller \" book is basically an \" oscar - winning \" book . May not have won the \" Best film \" , but even if you won the best director or best script , you \\'re still an \" oscar - winning \" film . Same thing for best sellers . Also , IIRC the rankings change every week or something like that . Some you might not be best seller one week , but you may be the next week . I guess even if you do n\\'t stay there for long , you still achieved the status . Hence , # 1 best seller .',\n",
       "  \"If you 're hearing about it , it 's because it was a very good or very well - publicized book ( or both ) , and almost every good or well - publicized book will be # 1 on the NY Times bestseller list for at least a little bit . Kindof like how almost every big or good movies are # 1 at the box office on their opening weekend .\",\n",
       "  \"One reason is lots of catagories . However , how the NY Times calculates its best seller list is n't comprehensive , and is pretty well understood by publishers . So publishers can [ buy a few books ] ( URL_0 ) in the right bookstores and send a book to the top of the list for at least a week .\"],\n",
       " 'chatgpt_answers': ['There are many different best seller lists that are published by various organizations, and the New York Times is just one of them. The New York Times best seller list is a weekly list that ranks the best-selling books in the United States based on sales data from a number of different retailers. The list is published in the New York Times newspaper and is widely considered to be one of the most influential best seller lists in the book industry. \\nIt\\'s important to note that the New York Times best seller list is not the only best seller list out there, and there are many other lists that rank the top-selling books in different categories or in different countries. So it\\'s possible that a book could be a best seller on one list but not on another. \\nAdditionally, the term \"best seller\" is often used more broadly to refer to any book that is selling well, regardless of whether it is on a specific best seller list or not. So it\\'s possible that you may hear about a book being a \"best seller\" even if it is not specifically ranked as a number one best seller on the New York Times list or any other list.'],\n",
       " 'source': 'reddit_eli5'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "639963bf-eeec-4819-9987-4e372c369a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:32.241981Z",
     "iopub.status.busy": "2023-07-11T00:37:32.241760Z",
     "iopub.status.idle": "2023-07-11T00:37:32.246562Z",
     "shell.execute_reply": "2023-07-11T00:37:32.245668Z",
     "shell.execute_reply.started": "2023-07-11T00:37:32.241959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17025 7297\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9b9401-2e2e-4f60-b3ed-74098bfe3073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:32.250563Z",
     "iopub.status.busy": "2023-07-11T00:37:32.250321Z",
     "iopub.status.idle": "2023-07-11T00:37:34.953638Z",
     "shell.execute_reply": "2023-07-11T00:37:34.952641Z",
     "shell.execute_reply.started": "2023-07-11T00:37:32.250540Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    data = []\n",
    "    for item in dataset:\n",
    "        for ans in item['human_answers']:\n",
    "            data.append({'text': ans, 'label': 'human'})\n",
    "        for ans in item['chatgpt_answers']:\n",
    "            data.append({'text': ans, 'label': 'chatgpt'})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_df = prepare_dataset(train_data)\n",
    "test_df = prepare_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a34d61-3587-4ca1-af94-2f4e0972eed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:34.957357Z",
     "iopub.status.busy": "2023-07-11T00:37:34.957110Z",
     "iopub.status.idle": "2023-07-11T00:37:34.970997Z",
     "shell.execute_reply": "2023-07-11T00:37:34.970083Z",
     "shell.execute_reply.started": "2023-07-11T00:37:34.957333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basically there are many categories of \" Best ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you 're hearing about it , it 's because it...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One reason is lots of catagories . However , h...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are many different best seller lists tha...</td>\n",
       "      <td>chatgpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>salt is good for not dying in car crashes and ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67643</th>\n",
       "      <td>On Reddit, \"meta\" refers to content that is ab...</td>\n",
       "      <td>chatgpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67644</th>\n",
       "      <td>Sure , religion accounts for the opposition to...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67645</th>\n",
       "      <td>The Bible also forbids theft , but it 's okay ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67646</th>\n",
       "      <td>If separation of church and state meant \" nobo...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67647</th>\n",
       "      <td>In the United States, the government and the c...</td>\n",
       "      <td>chatgpt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67648 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text    label\n",
       "0      Basically there are many categories of \" Best ...    human\n",
       "1      If you 're hearing about it , it 's because it...    human\n",
       "2      One reason is lots of catagories . However , h...    human\n",
       "3      There are many different best seller lists tha...  chatgpt\n",
       "4      salt is good for not dying in car crashes and ...    human\n",
       "...                                                  ...      ...\n",
       "67643  On Reddit, \"meta\" refers to content that is ab...  chatgpt\n",
       "67644  Sure , religion accounts for the opposition to...    human\n",
       "67645  The Bible also forbids theft , but it 's okay ...    human\n",
       "67646  If separation of church and state meant \" nobo...    human\n",
       "67647  In the United States, the government and the c...  chatgpt\n",
       "\n",
       "[67648 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd54a7a-0aeb-48b7-9710-cbe6676a1762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:37:34.974579Z",
     "iopub.status.busy": "2023-07-11T00:37:34.974281Z",
     "iopub.status.idle": "2023-07-11T00:38:04.825544Z",
     "shell.execute_reply": "2023-07-11T00:38:04.824784Z",
     "shell.execute_reply.started": "2023-07-11T00:37:34.974555Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a label mapping \n",
    "label2id = {'human':0, 'chatgpt':1}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def encode_examples(df):\n",
    "    encodings = tokenizer(df['text'].tolist(), truncation=True, padding='longest', max_length=512, return_tensors='pt')\n",
    "    labels = df['label'].map(label2id).tolist()\n",
    "    return {**encodings, 'labels': labels}\n",
    "\n",
    "train_encodings = encode_examples(train_df)\n",
    "test_encodings = encode_examples(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8eb4e60-a6ec-48b3-aa9f-2196f0b0385d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:38:04.829159Z",
     "iopub.status.busy": "2023-07-11T00:38:04.828728Z",
     "iopub.status.idle": "2023-07-11T00:38:04.834880Z",
     "shell.execute_reply": "2023-07-11T00:38:04.834145Z",
     "shell.execute_reply.started": "2023-07-11T00:38:04.829134Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataset class \n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items() if key != 'labels'}\n",
    "        item['labels'] = torch.tensor(self.encodings['labels'][idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(train_encodings)\n",
    "test_dataset = MyDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b316a9-4e84-437f-9286-67da6d9c350b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:38:04.839961Z",
     "iopub.status.busy": "2023-07-11T00:38:04.839497Z",
     "iopub.status.idle": "2023-07-11T00:38:08.047891Z",
     "shell.execute_reply": "2023-07-11T00:38:08.047088Z",
     "shell.execute_reply.started": "2023-07-11T00:38:04.839935Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup Trainer for training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    per_device_train_batch_size=16,  # Reduce batch size\n",
    "    gradient_accumulation_steps=4,  # Add gradient accumulation\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Specify the configuration\n",
    "config = RobertaConfig.from_pretrained('roberta-base')\n",
    "config.num_labels = len(label2id)  # Number of classes for classification\n",
    "config.gradient_checkpointing = True  # Enable gradient checkpointing\n",
    "\n",
    "# Create a new RoBERTa model for sequence classification\n",
    "model = RobertaForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa684d6-2549-456b-b0b3-5b02efdba106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T00:38:08.051425Z",
     "iopub.status.busy": "2023-07-11T00:38:08.050992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 67648\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 3171\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mohgodaaaa\u001b[0m (\u001b[33mshana\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20230711_003812-2zhtdk7s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/shana/huggingface/runs/2zhtdk7s\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/shana/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138/871423802.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items() if key != 'labels'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='3171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  15/3171 01:46 < 7:10:06, 0.12 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup Trainer for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          \n",
    "#     per_device_train_batch_size=64,\n",
    "#     num_train_epochs=3,  # You might want to increase this\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                 \n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,   \n",
    "    eval_dataset=test_dataset,          \n",
    "    data_collator=DataCollatorWithPadding(tokenizer), \n",
    "    tokenizer=tokenizer,  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c17823-9cd3-4bcb-af26-8b26209907c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
